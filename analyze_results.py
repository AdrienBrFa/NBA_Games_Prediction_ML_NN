"""
Script pour comparer les performances avant/aprÃ¨s les amÃ©liorations.
Peut aussi comparer avec les runs archivÃ©s.
"""

import json
import sys
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

# Ajouter le rÃ©pertoire scripts au path
sys.path.insert(0, str(Path(__file__).parent / "scripts"))

from scripts.archive_manager import list_archives, compare_archives, print_comparison

# Charger les rÃ©sultats actuels si disponibles
results_path = Path("outputs/results.json")

if results_path.exists():
    with open(results_path, 'r') as f:
        results = json.load(f)
    
    print("="*80)
    print("RÃ‰SUMÃ‰ DES PERFORMANCES DU MODÃˆLE")
    print("="*80)
    
    # Afficher le seuil optimal
    if 'optimal_threshold' in results:
        print(f"\nðŸŽ¯ Seuil optimal trouvÃ© : {results['optimal_threshold']:.3f}")
    
    # Tableau comparatif
    print("\nðŸ“Š MÃ‰TRIQUES PAR DATASET")
    print("-" * 80)
    print(f"{'MÃ©trique':<20} {'Train':>15} {'Validation':>15} {'Test':>15}")
    print("-" * 80)
    
    metrics_to_show = ['accuracy', 'auc', 'f1_score', 'log_loss']
    
    for metric in metrics_to_show:
        train_val = results['train_metrics'].get(metric, 'N/A')
        val_val = results['val_metrics'].get(metric, 'N/A')
        test_val = results['test_metrics'].get(metric, 'N/A')
        
        if isinstance(train_val, (int, float)):
            print(f"{metric.upper():<20} {train_val:>15.4f} {val_val:>15.4f} {test_val:>15.4f}")
        else:
            print(f"{metric.upper():<20} {str(train_val):>15} {str(val_val):>15} {str(test_val):>15}")
    
    print("-" * 80)
    
    # Analyse du surapprentissage
    overfitting_gap = results['train_metrics']['accuracy'] - results['test_metrics']['accuracy']
    print(f"\nðŸ“ˆ Gap Train-Test (Accuracy): {overfitting_gap:.4f} ({overfitting_gap*100:.2f}%)")
    
    if overfitting_gap < 0.05:
        print("   âœ… Excellent ! Peu de surapprentissage.")
    elif overfitting_gap < 0.10:
        print("   âš ï¸  Surapprentissage modÃ©rÃ©. ConsidÃ©rer plus de rÃ©gularisation.")
    else:
        print("   âŒ Surapprentissage important. Augmenter la rÃ©gularisation.")
    
    # Matrice de confusion test
    print("\nðŸŽ² MATRICE DE CONFUSION (Test Set)")
    print("-" * 80)
    cm = results['test_metrics']['confusion_matrix']
    print(f"                    Predicted")
    print(f"                Away Win    Home Win")
    print(f"Actual  Away Win    {cm[0][0]:<8}    {cm[0][1]:<8}")
    print(f"        Home Win    {cm[1][0]:<8}    {cm[1][1]:<8}")
    
    # Calculer prÃ©cision et rappel
    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    
    print(f"\n   PrÃ©cision (Home Win): {precision:.4f}")
    print(f"   Rappel (Home Win): {recall:.4f}")
    
    # Historique d'entraÃ®nement
    print("\nðŸ“š HISTORIQUE D'ENTRAÃŽNEMENT")
    print("-" * 80)
    print(f"Nombre d'epochs: {results['epochs_trained']}")
    print(f"Loss finale (train): {results['training_history']['loss'][-1]:.4f}")
    print(f"Loss finale (val): {results['training_history']['val_loss'][-1]:.4f}")
    print(f"Accuracy finale (train): {results['training_history']['accuracy'][-1]:.4f}")
    print(f"Accuracy finale (val): {results['training_history']['val_accuracy'][-1]:.4f}")
    
    # Recommandations
    print("\nðŸ’¡ RECOMMANDATIONS")
    print("="*80)
    
    if results['test_metrics']['accuracy'] < 0.60:
        print("1. L'accuracy est relativement faible (<60%)")
        print("   â†’ ConsidÃ©rer plus de features ou feature engineering")
        print("   â†’ Essayer un modÃ¨le plus profond [128, 64, 32]")
    
    test_f1 = results['test_metrics'].get('f1_score')
    test_acc = results['test_metrics']['accuracy']
    
    if test_f1 is not None and test_f1 < test_acc - 0.05:
        print("2. F1-score significativement infÃ©rieur Ã  l'accuracy")
        print("   â†’ Le modÃ¨le favorise une classe (probablement Home Win)")
        print("   â†’ Essayer class_weight pour Ã©quilibrer")
    
    if overfitting_gap > 0.08:
        print("3. Surapprentissage dÃ©tectÃ©")
        print("   â†’ Augmenter L2 reg (ex: 0.005 ou 0.01)")
        print("   â†’ Augmenter Dropout (ex: 0.4 ou 0.5)")
        print("   â†’ RÃ©duire la taille du modÃ¨le")
    
    print("\n" + "="*80)
    
    # Comparer avec la derniÃ¨re archive si disponible
    print("\n")
    archives = list_archives()
    if archives:
        print("ðŸ”„ COMPARAISON AVEC LE RUN PRÃ‰CÃ‰DENT")
        comparison = compare_archives('latest')
        if comparison:
            print_comparison(comparison)
    
else:
    print("âŒ Aucun fichier results.json trouvÃ©.")
    print("   Veuillez d'abord exÃ©cuter : python run_pipeline.py")

# Liste des archives disponibles
print("\n" + "="*80)
print("ðŸ“¦ ARCHIVES DISPONIBLES")
print("="*80)

archives = list_archives()
if archives:
    print(f"\nNombre total d'archives: {len(archives)}\n")
    for i, archive in enumerate(archives[:5], 1):  # Afficher les 5 plus rÃ©centes
        print(f"{i}. {archive['archive_timestamp']}")
        if archive.get('original_results'):
            res = archive['original_results']
            acc = res.get('test_accuracy')
            auc = res.get('test_auc')
            f1 = res.get('test_f1')
            if acc is not None:
                print(f"   Accuracy: {acc:.4f}", end="")
            if auc is not None:
                print(f" | AUC: {auc:.4f}", end="")
            if f1 is not None:
                print(f" | F1: {f1:.4f}", end="")
            print()
    
    if len(archives) > 5:
        print(f"\n... et {len(archives) - 5} autre(s) archive(s)")
    
    print("\nPour comparer des runs spÃ©cifiques:")
    print("  python scripts/archive_manager.py --compare")
    print("  python scripts/archive_manager.py --list")
else:
    print("\nAucune archive trouvÃ©e. Les rÃ©sultats seront archivÃ©s au prochain run.")

print("="*80)
